{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9244746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from threading import Thread\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    sys.path.append(glob.glob('carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "import carla\n",
    "from simSync import SimulatorSynchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b05990c",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fca6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 15\n",
    "\n",
    "SHOW_PREVIEW = False\n",
    "SECONDS_PER_EPISODE = 50\n",
    "REPLAY_MEMORY_SIZE = 5_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000\n",
    "MINIBATCH_SIZE = 16\n",
    "PREDICTION_BATCH_SIZE = 1\n",
    "TRAINING_BATCH_SIZE = MINIBATCH_SIZE // 4\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "MIN_REWARD = -20000\n",
    "\n",
    "MODEL_NAME = \"SqueezeNet\"\n",
    "LAST_CHECKPOINT = 0\n",
    "CHECKPOINT_INTERVAL = 20*60\n",
    "MODEL_CHECKPOINT = \"\"\n",
    "LOAD_CHECKPOINT = False\n",
    "\n",
    "[IM_H, IM_W] = [224, 224]\n",
    "ACTION_SPACE_SIZE = 11\n",
    "STEER_LIMIT = 0.3\n",
    "\n",
    "MEMORY_FRACTION = 0.8\n",
    "\n",
    "EPISODES = 10000\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "epsilon = 1.0\n",
    "EPSILON_DECAY = 0.997 ## 0.9975 99975\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72b57f",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becc27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sim = SimulatorSynchronous(fps=FPS, no_agents=1, port=2000)\n",
    "        self.sim.spawn_agents()\n",
    "        self.sim.agent[0].attach_controller()\n",
    "        \n",
    "        self.steering_options = np.linspace(-STEER_LIMIT, STEER_LIMIT, ACTION_SPACE_SIZE)\n",
    "        \n",
    "        self.step_no = 0\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.sim.agent[0].waypoint = self.sim.map.get_waypoint(self.sim.agent[0].vehicle.get_location(),\n",
    "                            project_to_road=True, \n",
    "                            lane_type=(carla.LaneType.Driving)) # carla.LaneType.Sidewalk\n",
    "\n",
    "        self.sim.agent[0].vehicle.set_transform(self.sim.agent[0].waypoint.transform)\n",
    "\n",
    "        self.step_no = 0\n",
    "\n",
    "        self.sim.agent[0].controller.set_target_velocity(30)\n",
    "        \n",
    "        self.sim.agent[0].collision = []\n",
    "        self.sim.world.tick()\n",
    "        \n",
    "        h, w, c = self.sim.agent[0].image.shape\n",
    "        \n",
    "        state = self.sim.agent[0].image[int(h/2) :, :, :]\n",
    "        \n",
    "        return state\n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        v = self.sim.agent[0].vehicle.get_velocity()\n",
    "        current_velocity = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "        \n",
    "        accel, decel = self.sim.agent[0].controller.velocity_controller(1/FPS, current_velocity)\n",
    "        \n",
    "        turn = self.steering_options[action]\n",
    "        \n",
    "        self.sim.agent[0].vehicle.apply_control(carla.VehicleControl(throttle=accel, brake=decel, steer=turn))\n",
    "\n",
    "        self.sim.world.tick()\n",
    "        \n",
    "        [distance, reached_waypoint] = self.sim.agent[0].check_waypoint()\n",
    "\n",
    "        if reached_waypoint:\n",
    "            reward += 2.0\n",
    "            self.sim.agent[0].update_waypoint()\n",
    "        else:\n",
    "            reward -= distance\n",
    "        \n",
    "        if distance > 4:\n",
    "            reward -= 500.0\n",
    "            \n",
    "        if len(self.sim.agent[0].collision) != 0:\n",
    "            reward -= 2000.0\n",
    "            self.sim.agent[0].collision = []\n",
    "\n",
    "        self.step_no += 1\n",
    "        done = True if (self.step_no >= SECONDS_PER_EPISODE*FPS) else False\n",
    "        \n",
    "        h, w, c = self.sim.agent[0].image.shape\n",
    "        \n",
    "        state = self.sim.agent[0].image[int(h/2) :, :, :]\n",
    "        \n",
    "        return state, reward, done, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21a71f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bed71a",
   "metadata": {},
   "source": [
    "### sample execution (requires torchvision)\n",
    "\n",
    "```\n",
    "model = models.squeezenet1_1(pretrained=False, num_classes=ACTION_SPACE_SIZE)\n",
    "\n",
    "input_image = cv2.imread(\"wall.jpg\")\n",
    "input_image = cv2.resize(input_image, (IM_H, IM_W), interpolation = cv2.INTER_AREA)\n",
    "input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "input_image = Image.fromarray(input_image)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    # transforms.Resize(256),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape ACTION_SPACE_SIZE, with confidence scores \n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)\n",
    "\n",
    "plt.imshow(input_tensor.permute(1, 2, 0))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b76050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    # transforms.Resize(256),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def process_image(img):\n",
    "    img = cv2.resize(img, (IM_H, IM_W), interpolation = cv2.INTER_AREA)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    input_tensor = preprocess(img)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce684c",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770de1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        print(\"Using \" + str(self.device))\n",
    "\n",
    "        self.model = models.squeezenet1_1(pretrained=False, num_classes=ACTION_SPACE_SIZE)\n",
    "        self.target_model = models.squeezenet1_1(pretrained=False, num_classes=ACTION_SPACE_SIZE)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.model.to(self.device)\n",
    "        self.target_model.to(self.device)\n",
    "        pytorch_total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(\"Trainable parameters: \" + str(pytorch_total_params))\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "        self.terminate = False\n",
    "        self.last_logged_episode = 0\n",
    "        self.training_initialized = False\n",
    "        self.step = 0\n",
    "        self.train_no = 0\n",
    "        self.best_reward = -10000\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (current_state, action, reward, new_state, done)\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        self.train_no += 1\n",
    "        \n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        current_states = torch.stack([process_image(transition[0]) for transition in minibatch])\n",
    "        with torch.no_grad():\n",
    "            current_qs_list = self.model(current_states.to(self.device)).cpu().detach().numpy()\n",
    "\n",
    "        new_current_states = torch.stack([process_image(transition[3]) for transition in minibatch])\n",
    "        with torch.no_grad():\n",
    "            future_qs_list = self.target_model(new_current_states.to(self.device)).cpu().detach().numpy()\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            X.append(process_image(current_state))\n",
    "            y.append(current_qs)\n",
    "\n",
    "        X = torch.stack(X)\n",
    "        y = torch.Tensor(np.array(y))\n",
    "        \n",
    "        log_this_step = False\n",
    "        if self.step > self.last_logged_episode:\n",
    "            log_this_step = True\n",
    "            self.last_log_episode = self.step\n",
    "\n",
    "        \"\"\" Train and Fit Model \"\"\"\n",
    "        # print(\"Train iter no \" + str(self.train_no))\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.model(X.to(self.device)).cpu()\n",
    "        y_pred = y_pred.cpu()\n",
    "        loss = self.loss_function(y_pred, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if log_this_step:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        state = process_image(state).unsqueeze(0) # convert image to tensor of size 1 x 3 x IM_H x IM_W\n",
    "        with torch.no_grad():\n",
    "            prediction = self.target_model(state.to(self.device)).cpu().detach().numpy()\n",
    "        return prediction[0]\n",
    "\n",
    "    def train_in_loop(self):\n",
    "        self.training_initialized = True\n",
    "\n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                return\n",
    "            self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ef9a2",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044160b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whoop up the carla server\n",
    "os.system('cmd /k \"\\\"F:\\Autonomous Vehicles\\CARLA-SImulator\\CARLA_0.9.11_windows\\CarlaUE4.exe\\\" -carla-port=2000 -quality-level=Epic\"')\n",
    "time.sleep(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Simulation\n",
      "Using cuda\n",
      "Trainable parameters: 728139\n",
      "Establishing Connection to Server\n",
      "Probably connected not sure tho\n",
      "Vehicle 0 spawned\n",
      "Survived /7.13 sec, episode 1 of 10000, train iter 0\n",
      "Survived /3.07 sec, episode 2 of 10000, train iter 0\n",
      "Survived /3.13 sec, episode 3 of 10000, train iter 0\n",
      "Survived /6.67 sec, episode 4 of 10000, train iter 0\n",
      "Survived /4.60 sec, episode 5 of 10000, train iter 0\n",
      "Survived /3.07 sec, episode 6 of 10000, train iter 0\n",
      "Survived /3.13 sec, episode 7 of 10000, train iter 0\n",
      "Survived /3.13 sec, episode 8 of 10000, train iter 0\n",
      "Survived /3.13 sec, episode 9 of 10000, train iter 0\n",
      "Survived /4.07 sec, episode 10 of 10000, train iter 0\n",
      "Survived /5.33 sec, episode 11 of 10000, train iter 0\n",
      "Survived /3.07 sec, episode 12 of 10000, train iter 0\n",
      "Survived /3.13 sec, episode 13 of 10000, train iter 0\n",
      "Survived /3.13 sec, episode 14 of 10000, train iter 0\n",
      "Survived /5.13 sec, episode 15 of 10000, train iter 0\n",
      "Survived /4.73 sec, episode 16 of 10000, train iter 0\n",
      "Survived /5.87 sec, episode 17 of 10000, train iter 22\n",
      "Survived /3.07 sec, episode 18 of 10000, train iter 39\n",
      "Survived /3.80 sec, episode 19 of 10000, train iter 62\n",
      "Survived /3.13 sec, episode 20 of 10000, train iter 78\n",
      "Survived /3.13 sec, episode 21 of 10000, train iter 95\n",
      "Survived /3.13 sec, episode 22 of 10000, train iter 112\n"
     ]
    }
   ],
   "source": [
    "print(\"Beginning Simulation\")\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "# Create models folder\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Create agent and environment\n",
    "agent = DQNAgent()\n",
    "env = CarEnv()\n",
    "\n",
    "if LOAD_CHECKPOINT is True:\n",
    "    agent.model.load_state_dict(torch.load(MODEL_CHECKPOINT))\n",
    "    agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "\n",
    "# Start training thread and wait for training to be initialized\n",
    "trainer_thread = Thread(target=agent.train_in_loop, daemon=True)\n",
    "trainer_thread.start()\n",
    "while not agent.training_initialized:\n",
    "    time.sleep(0.01)\n",
    "\n",
    "ep_rewards = []\n",
    "avg_rewards = []\n",
    "max_rewards = []\n",
    "min_rewards = []\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in range(1, EPISODES + 1):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    episode_start = time.time()\n",
    "\n",
    "    # Play for given number of seconds only\n",
    "    while True:\n",
    "\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, ACTION_SPACE_SIZE)\n",
    "            # This takes no time, so we add a delay matching 60 FPS (prediction above takes longer)\n",
    "            time.sleep(1/FPS)\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        if  SHOW_PREVIEW:\n",
    "            print(reward)\n",
    "            cv2.imshow(\"Agent Cam\", new_state)\n",
    "            cv2.waitKey(1)\n",
    "        \n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Every step we update replay memory\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "        if done or (episode_reward <= MIN_REWARD):\n",
    "            break\n",
    "\n",
    "    # End of episode \n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "\n",
    "    print(f'Survived {step/FPS:_>.2f} sec, episode {episode} of {EPISODES}, train iter {agent.train_no}')\n",
    "    \n",
    "    # Save model, but only when min reward is greater or equal a set value\n",
    "    if episode_reward >= agent.best_reward or (time.time() - LAST_CHECKPOINT) > CHECKPOINT_INTERVAL:\n",
    "        LAST_CHECKPOINT = time.time()\n",
    "        if episode_reward >= agent.best_reward:\n",
    "            agent.best_reward = episode_reward\n",
    "        torch.save(agent.model.state_dict(),\n",
    "                   f'models/{MODEL_NAME}__{episode_reward:_>7.2f}reward_{int(time.time())}.pt')\n",
    "\n",
    "# Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "    file_object = open('rewards.txt', 'a')\n",
    "    file_object.write(str(episode_reward))\n",
    "    file_object.write(', ')\n",
    "    file_object.close()\n",
    "\n",
    "if SHOW_PREVIEW:    \n",
    "    cv2.destroy_all_windows()\n",
    "# Set termination flag for training thread and wait for it to finish\n",
    "agent.terminate = True\n",
    "trainer_thread.join()\n",
    "\n",
    "print(\"Finished training model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
