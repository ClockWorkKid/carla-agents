{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bb4f8f",
   "metadata": {},
   "source": [
    "\n",
    "In this code, a single DQN agent explores inside the carla simulator running in asynchronous mode. \n",
    "* The DNN used to approximate the Q-function is a custom made network (no reason, any specific network can be used). \n",
    "* The input image size has been set to 640x480\n",
    "* Action space size is 3, action values being (constant throttle, fixed left), (constant throttle, straight) and\n",
    "(constant throttle, fixed right)\n",
    "* Two threads are used for 1. The agent exploring the environment 2. The model training\n",
    "* The reward function is adjusted to detect two major events 1. Huge penalty for collision 2. Smaller penalty for lane\n",
    "crossing, 3. Very small positive reward for no other events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import gc\n",
    "\n",
    "from threading import Thread\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    sys.path.append(glob.glob('carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "import carla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7b823",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ec1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PREVIEW = False\n",
    "IM_WIDTH = 640\n",
    "IM_HEIGHT = 480\n",
    "SECONDS_PER_EPISODE = 50\n",
    "REPLAY_MEMORY_SIZE = 5_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000\n",
    "MINIBATCH_SIZE = 16\n",
    "PREDICTION_BATCH_SIZE = 1\n",
    "TRAINING_BATCH_SIZE = MINIBATCH_SIZE // 4\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "MODEL_NAME = \"Custom\"\n",
    "LAST_CHECKPOINT = 0\n",
    "CHECKPOINT_INTERVAL = 20*60\n",
    "MODEL_CHECKPOINT = \"models/Custom__-267.40reward_1620490097.pt\"\n",
    "LOAD_CHECKPOINT = False\n",
    "\n",
    "MEMORY_FRACTION = 0.8\n",
    "\n",
    "EPISODES = 5000\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "epsilon = 1.0\n",
    "EPSILON_DECAY = 0.997 ## 0.9975 99975\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb0fb9",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "    SHOW_CAM = SHOW_PREVIEW\n",
    "    STEER_AMT = 1.0\n",
    "    im_width = IM_WIDTH\n",
    "    im_height = IM_HEIGHT\n",
    "    front_camera = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.get_world()\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = self.blueprint_library.filter(\"model3\")[0]\n",
    "\n",
    "    def reset(self):\n",
    "        self.collision_hist = []\n",
    "        self.lane_hist = 0\n",
    "        self.actor_list = []\n",
    "\n",
    "        self.transform = random.choice(self.world.get_map().get_spawn_points())\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, self.transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "\n",
    "        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        self.rgb_cam.set_attribute(\"image_size_x\", f\"{self.im_width}\")\n",
    "        self.rgb_cam.set_attribute(\"image_size_y\", f\"{self.im_height}\")\n",
    "        self.rgb_cam.set_attribute(\"fov\", f\"110\")\n",
    "\n",
    "        transform = carla.Transform(carla.Location(x=2.5, z=0.7))\n",
    "        self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_img(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        time.sleep(4)\n",
    "\n",
    "        colsensor = self.blueprint_library.find(\"sensor.other.collision\")\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.colsensor)\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "\n",
    "        lane_bp = self.blueprint_library.find('sensor.other.lane_invasion')\n",
    "        self.lane_sensor = self.world.spawn_actor(lane_bp, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.lane_sensor)\n",
    "        self.lane_sensor.listen(lambda lane: self.lane_callback(lane))\n",
    "\n",
    "        while self.front_camera is None:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        spectator = self.world.get_spectator()\n",
    "        world_snapshot = self.world.wait_for_tick()\n",
    "        spectator.set_transform(self.vehicle.get_transform())\n",
    "        self.episode_start = time.time()\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.5, brake=0.0))\n",
    "\n",
    "        return self.front_camera\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def lane_callback(self, lane):\n",
    "        self.lane_hist += 1\n",
    "\n",
    "    def process_img(self, image):\n",
    "        i = np.array(image.raw_data)\n",
    "        #print(i.shape)\n",
    "        i2 = i.reshape((self.im_height, self.im_width, 4))\n",
    "        i3 = i2[:, :, :3]\n",
    "        if self.SHOW_CAM:\n",
    "            cv2.imshow(\"\", i3)\n",
    "            cv2.waitKey(1)\n",
    "        self.front_camera = i3\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=0.5, steer=-0.3))\n",
    "        elif action == 1:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=0.5, steer= 0))\n",
    "        elif action == 2:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=0.5, steer=0.3))\n",
    "\n",
    "        # v = self.vehicle.get_velocity()\n",
    "        # kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "\n",
    "        done = False\n",
    "        if len(self.collision_hist) != 0:\n",
    "            reward = -1000\n",
    "            done = True\n",
    "        elif self.lane_hist > 0:\n",
    "            reward = -100 * self.lane_hist\n",
    "            self.lane_hist = 0\n",
    "        else:\n",
    "            reward = 0.2\n",
    "\n",
    "        if self.episode_start + SECONDS_PER_EPISODE < time.time():\n",
    "            done = True\n",
    "\n",
    "        return self.front_camera, reward, done, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8b896",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size=(3, 10, 10), output_size=4):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        shape = np.array([input_size[1], input_size[2]]).astype(np.int32)\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_size[0], out_channels=32, kernel_size=3)\n",
    "        shape = shape - 2\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        shape = shape // 2\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        shape = shape - 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        shape = shape // 2\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.dens1 = nn.Linear(in_features=32 * shape[0] * shape[1], out_features=64)\n",
    "        self.dens2 = nn.Linear(64, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = x.reshape((x.size(0), -1))\n",
    "        x = self.dens1(x)\n",
    "\n",
    "        x = self.dens2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad90a16",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        print(\"Using \" + str(self.device))\n",
    "\n",
    "        self.model = NeuralNet(input_size=(3, IM_HEIGHT, IM_WIDTH), output_size=3)\n",
    "        self.target_model = NeuralNet(input_size=(3, IM_HEIGHT, IM_WIDTH), output_size=3)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.model.to(self.device)\n",
    "        self.target_model.to(self.device)\n",
    "        pytorch_total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(\"Trainable parameters: \" + str(pytorch_total_params))\n",
    "\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "        self.terminate = False\n",
    "        self.last_logged_episode = 0\n",
    "        self.training_initialized = False\n",
    "        self.step = 0\n",
    "        self.train_no = 0\n",
    "        self.best_reward = -10000\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (current_state, action, reward, new_state, done)\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        self.train_no += 1\n",
    "        # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        current_states = np.array([transition[0] for transition in minibatch]) / 255\n",
    "        current_states = np.swapaxes(current_states, 1, 3)\n",
    "        current_states = np.swapaxes(current_states, 2, 3)\n",
    "        current_states = torch.Tensor(current_states.astype(np.float16))\n",
    "        with torch.no_grad():\n",
    "            current_qs_list = self.model(current_states.to(self.device))\n",
    "        current_qs_list = current_qs_list.cpu().detach().numpy()\n",
    "\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch]) / 255\n",
    "        new_current_states = np.swapaxes(new_current_states, 1, 3)\n",
    "        new_current_states = np.swapaxes(new_current_states, 2, 3)\n",
    "        new_current_states = torch.Tensor(new_current_states.astype(np.float16))\n",
    "        with torch.no_grad():\n",
    "            future_qs_list = self.target_model(new_current_states.to(self.device))\n",
    "        future_qs_list = future_qs_list.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            X.append(np.rollaxis(current_state, 2, 0)/255)\n",
    "            y.append(current_qs)\n",
    "\n",
    "\n",
    "        X = torch.Tensor(np.array(X).astype(np.float16))\n",
    "        y = torch.Tensor(np.array(y))\n",
    "        log_this_step = False\n",
    "        if self.step > self.last_logged_episode:\n",
    "            log_this_step = True\n",
    "            self.last_log_episode = self.step\n",
    "\n",
    "        \"\"\" Train and Fit Model \"\"\"\n",
    "        print(\"Train iter no \" + str(self.train_no))\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.model(X.to(self.device)).cpu()\n",
    "        y_pred = y_pred.cpu()\n",
    "        loss = self.loss_function(y_pred, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if log_this_step:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        state = np.rollaxis(state, 2, 0)  # shape of (H, W, C) to (C, H, W)\n",
    "        state = np.array(state).reshape(-1, *state.shape)  # (C, H, W) to (1, C, H, W)\n",
    "        state = state.astype(np.float16)\n",
    "        state = torch.Tensor(state / 255)\n",
    "        with torch.no_grad():\n",
    "            prediction = self.target_model(state.to(self.device)).cpu().detach().numpy()\n",
    "        return prediction[0]\n",
    "\n",
    "    def train_in_loop(self):\n",
    "        self.training_initialized = True\n",
    "\n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                return\n",
    "            self.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acea40",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc886e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning Simulation\")\n",
    "\n",
    "FPS = 60\n",
    "# For stats\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "# Create models folder\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Create agent and environment\n",
    "agent = DQNAgent()\n",
    "env = CarEnv()\n",
    "\n",
    "if LOAD_CHECKPOINT is True:\n",
    "    agent.model.load_state_dict(torch.load(MODEL_CHECKPOINT))\n",
    "    agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "\n",
    "# Start training thread and wait for training to be initialized\n",
    "trainer_thread = Thread(target=agent.train_in_loop, daemon=True)\n",
    "trainer_thread.start()\n",
    "while not agent.training_initialized:\n",
    "    time.sleep(0.01)\n",
    "\n",
    "# Initialize predictions - first prediction takes longer as of initialization that has to be done\n",
    "# It's better to do a first prediction then before we start iterating over episode steps\n",
    "# agent.get_qs(np.ones((env.im_height, env.im_width, 3)))\n",
    "\n",
    "ep_rewards = []\n",
    "avg_rewards = []\n",
    "max_rewards = []\n",
    "min_rewards = []\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    env.collision_hist = []\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    episode_start = time.time()\n",
    "\n",
    "    # Play for given number of seconds only\n",
    "    while True:\n",
    "\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, 3)\n",
    "            # This takes no time, so we add a delay matching 60 FPS (prediction above takes longer)\n",
    "            time.sleep(1/FPS)\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Every step we update replay memory\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # End of episode - destroy agents\n",
    "    for actor in env.actor_list:\n",
    "        actor.destroy()\n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    # if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "    #     average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "    #     min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "    #     max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "    #     avg_rewards.append(average_reward)\n",
    "    #     max_rewards.append(max_reward)\n",
    "    #     min_rewards.append(min_reward)\n",
    "\n",
    "    # Save model, but only when min reward is greater or equal a set value\n",
    "    if episode_reward >= agent.best_reward or (time.time() - LAST_CHECKPOINT) > CHECKPOINT_INTERVAL:\n",
    "        LAST_CHECKPOINT = time.time()\n",
    "        if episode_reward >= agent.best_reward:\n",
    "            agent.best_reward = episode_reward\n",
    "        torch.save(agent.model.state_dict(),\n",
    "                   f'models/{MODEL_NAME}__{episode_reward:_>7.2f}reward_{int(time.time())}.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "    file_object = open('rewards.txt', 'a')\n",
    "    file_object.write(str(episode_reward))\n",
    "    file_object.write(', ')\n",
    "    file_object.close()\n",
    "\n",
    "# Set termination flag for training thread and wait for it to finish\n",
    "agent.terminate = True\n",
    "trainer_thread.join()\n",
    "\n",
    "print(\"Finished training model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
